<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kuan-Wei Ho (Tommy Ho) on Kuan-Wei Ho (Tommy Ho)</title>
    <link>/</link>
    <description>Recent content in Kuan-Wei Ho (Tommy Ho) on Kuan-Wei Ho (Tommy Ho)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Mon, 31 Dec 2018 00:00:00 +0800</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Nvidia</title>
      <link>/talk/nvidia/</link>
      <pubDate>Sat, 05 Jan 2019 23:27:12 +0800</pubDate>
      
      <guid>/talk/nvidia/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AI Storyteller</title>
      <link>/project/ai-storyteller/</link>
      <pubDate>Tue, 01 Jan 2019 17:22:31 +0800</pubDate>
      
      <guid>/project/ai-storyteller/</guid>
      <description>&lt;p&gt;In this project, we design an AI model to generate believable Chinese stories based on MIT Media Lab&amp;rsquo;s &lt;a href=&#34;http://conceptnet.io&#34; target=&#34;_blank&#34;&gt;ConceptNet&lt;/a&gt; semantic database. The project is seperated into two parts. The first part is to use &lt;strong&gt;Monte Carlo tree search&lt;/strong&gt; to generate  believable commonsense words sequence based on the semantic information provided by ConceptNet; the second part is to translate the sequence composed of commonsense Chinese words and relations into natural Chinese language sentences.&lt;/p&gt;

&lt;p&gt;The first part of the project is based on the previous work of &lt;a href=&#34;https://aaai.org/ocs/index.php/AIIDE/AIIDE16/paper/view/13993/13583&#34; target=&#34;_blank&#34;&gt;Generate Believable Causal Plots with User Preferences Using Constrained Monte Carlo Tree Search&lt;/a&gt;, but in different way. First, we implement the idea on Chinese dataset, which brings more chanllenges such as customized pre-trained word embedding. Second, we further extend the work by proposing using &lt;strong&gt;sequence-to-sequence learning (seq2seq)&lt;/strong&gt; to translate the Chinese words and semantic relations into natural language, instead of the rule-based method used in the work.&lt;/p&gt;

&lt;p&gt;The second part of the project is to train seq2seq to learn the mapping between the sequences generated by Monte Carlo tree search in the first part and the actual natural language sentences. The seq2seq implementation can be found in my &lt;a href=&#34;https://github.com/Thanatine/TensorFlow-Seq2Seq&#34; target=&#34;_blank&#34;&gt;GitHub repo&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ARGAN</title>
      <link>/project/argan/</link>
      <pubDate>Tue, 01 Jan 2019 16:55:09 +0800</pubDate>
      
      <guid>/project/argan/</guid>
      <description>&lt;p&gt;Image-to-image translation has been a popular topic since the Deep Learning had taken over a important position in research field regarding Computer Vision. In such kind of task, the model will usually learn a mapping &lt;code&gt;G : X → Y&lt;/code&gt; such that the distribution of generated images from &lt;code&gt;G(X)&lt;/code&gt; is indistinguishable from the distribution &lt;code&gt;Y&lt;/code&gt;. What we have achieved in this project is having aids from additional image encoded with input, to assist the model find the mapping more easily to the desired output. Such thought can be written as &lt;code&gt;G : X + Z → Y&lt;/code&gt; . We successfully enrich the complexity of &lt;code&gt;X&lt;/code&gt; with the help of &lt;code&gt;Z&lt;/code&gt;, and map &lt;code&gt;G(X + Z)&lt;/code&gt; with &lt;code&gt;Y&lt;/code&gt; by this implementation. Interesting results are generated by this approach.&lt;/p&gt;

&lt;p&gt;Our project is inspired by SAGOSKATT, which is a campaign held by IKEA meant to produce toys all designed by children for charity.&lt;/p&gt;

&lt;p&gt;The project is also the 3rd prize winner in Final Project Competition of National Tsing Hua University The Cutting-Edge of Deep Learning (CEDL 2017), sponsored by QCT and HTC Health.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Twitter Sarcasm Detection</title>
      <link>/project/twitter-sarcasm-detection/</link>
      <pubDate>Tue, 01 Jan 2019 16:54:46 +0800</pubDate>
      
      <guid>/project/twitter-sarcasm-detection/</guid>
      <description>&lt;p&gt;Precise semantic presentation of a sentence and definitive information extraction are key steps in the accurate processing of sentence meaning, especially for figurative phenomena such as sarcasm, irony and metaphor. Semantic modeling faces a new challenge in social media because grammatical inaccuracy is commonplace yet many previous state-of-the-art methods exploit grammatical structure. For sarcasm detection over social media content, researchers so far have counted on Bag-of-Words, N-grams and etc. In this project, we apply a neural network semantic model for sarcasm detection on &lt;a href=&#34;arxiv.org/abs/1704.05579&#34; target=&#34;_blank&#34;&gt;Twitter sarcasm dataset&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We propose to &lt;strong&gt;combine CNN(1D) with LSTM&lt;/strong&gt; in this project. CNN(1D) extract proper local feature extraction which resembles 1-gram. After the feature extraction, LSTM can fully learn the hidden information behind the context of sequences of higher level features provided by CNN(1D). Such design is often seen in tasks requiring comprehending sequences of images or sounds, such as activity classification or sound classification. We believe that the mixture of CNNs and RNNs can work well also in semantic analysis, as the architecture utilizes both networks’ advantages.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visual Relationship Prediction via Label Clustering and Incorporation of Depth Information</title>
      <link>/publication/pic-workshop/</link>
      <pubDate>Mon, 31 Dec 2018 20:54:49 +0800</pubDate>
      
      <guid>/publication/pic-workshop/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;
</description>
    </item>
    
  </channel>
</rss>
